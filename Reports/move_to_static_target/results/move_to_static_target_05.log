
            ┐  ╖
        ╓╖╬│╡  ││╬╖╖
    ╓╖╬│││││┘  ╬│││││╬╖
 ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗
 ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣
 ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣
 ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣
 ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜
 ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣
 ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣
   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣
      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜
          ╙╬╬╬╣╣╣╜
             ╙
        
 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 1.13.1+cu117
[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
[INFO] Connected to Unity environment with package version 2.3.0-exp.3 and communication version 1.5.0
[INFO] Connected new brain: move_to_static_target_05?team=0
[INFO] Hyperparameters for behavior name move_to_static_target_05: 
	trainer_type:	ppo
	hyperparameters:	
	  batch_size:	256
	  buffer_size:	8192
	  learning_rate:	0.0003
	  beta:	0.005
	  epsilon:	0.2
	  lambd:	0.95
	  num_epoch:	3
	  shared_critic:	False
	  learning_rate_schedule:	linear
	  beta_schedule:	linear
	  epsilon_schedule:	linear
	network_settings:	
	  normalize:	False
	  hidden_units:	512
	  num_layers:	1
	  vis_encode_type:	simple
	  memory:	None
	  goal_conditioning_type:	hyper
	  deterministic:	False
	reward_signals:	
	  extrinsic:	
	    gamma:	0.9
	    strength:	1.0
	    network_settings:	
	      normalize:	False
	      hidden_units:	128
	      num_layers:	2
	      vis_encode_type:	simple
	      memory:	None
	      goal_conditioning_type:	hyper
	      deterministic:	False
	  curiosity:	
	    gamma:	0.99
	    strength:	0.02
	    network_settings:	
	      normalize:	False
	      hidden_units:	256
	      num_layers:	3
	      vis_encode_type:	simple
	      memory:	None
	      goal_conditioning_type:	hyper
	      deterministic:	False
	    learning_rate:	0.0003
	    encoding_size:	None
	init_path:	None
	keep_checkpoints:	100
	checkpoint_interval:	500000
	max_steps:	10000000
	time_horizon:	256
	summary_freq:	50000
	threaded:	False
	self_play:	None
	behavioral_cloning:	None
/home/mih/.local/share/virtualenvs/ml_agents_test-Mi5MBi39/lib/python3.10/site-packages/mlagents/trainers/torch_entities/utils.py:289: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3277.)
  torch.nn.functional.one_hot(_act.T, action_size[i]).float()
[INFO] move_to_static_target_05. Step: 50000. Time Elapsed: 77.475 s. Mean Reward: -55.820. Std of Reward: 5.369. Training.
[INFO] move_to_static_target_05. Step: 100000. Time Elapsed: 145.659 s. Mean Reward: -55.710. Std of Reward: 10.854. Training.
[INFO] move_to_static_target_05. Step: 150000. Time Elapsed: 212.521 s. Mean Reward: -55.623. Std of Reward: 10.448. Training.
[INFO] move_to_static_target_05. Step: 200000. Time Elapsed: 277.229 s. Mean Reward: -55.412. Std of Reward: 0.094. Training.
[INFO] move_to_static_target_05. Step: 250000. Time Elapsed: 344.069 s. Mean Reward: -55.412. Std of Reward: 0.094. Training.
[INFO] move_to_static_target_05. Step: 300000. Time Elapsed: 410.910 s. Mean Reward: -55.412. Std of Reward: 0.093. Training.
[INFO] move_to_static_target_05. Step: 350000. Time Elapsed: 478.016 s. Mean Reward: -55.412. Std of Reward: 0.093. Training.
[INFO] move_to_static_target_05. Step: 400000. Time Elapsed: 544.786 s. Mean Reward: -55.412. Std of Reward: 0.094. Training.
[INFO] move_to_static_target_05. Step: 450000. Time Elapsed: 608.675 s. Mean Reward: -55.412. Std of Reward: 0.093. Training.
[INFO] move_to_static_target_05. Step: 500000. Time Elapsed: 673.508 s. Mean Reward: -55.412. Std of Reward: 0.094. Training.
[INFO] Exported results/move_to_static_target_05/move_to_static_target_05/move_to_static_target_05-499984.onnx
[INFO] move_to_static_target_05. Step: 550000. Time Elapsed: 738.234 s. Mean Reward: -55.412. Std of Reward: 0.093. Training.
[INFO] move_to_static_target_05. Step: 600000. Time Elapsed: 802.985 s. Mean Reward: -55.412. Std of Reward: 0.093. Training.
[INFO] move_to_static_target_05. Step: 650000. Time Elapsed: 866.668 s. Mean Reward: -55.412. Std of Reward: 0.094. Training.
[INFO] move_to_static_target_05. Step: 700000. Time Elapsed: 929.897 s. Mean Reward: -55.413. Std of Reward: 0.093. Training.
[INFO] move_to_static_target_05. Step: 750000. Time Elapsed: 993.108 s. Mean Reward: -55.412. Std of Reward: 0.094. Training.
[INFO] move_to_static_target_05. Step: 800000. Time Elapsed: 1056.613 s. Mean Reward: -55.412. Std of Reward: 0.094. Training.
[INFO] move_to_static_target_05. Step: 850000. Time Elapsed: 1119.993 s. Mean Reward: -55.413. Std of Reward: 0.093. Training.
[INFO] move_to_static_target_05. Step: 900000. Time Elapsed: 1183.328 s. Mean Reward: -55.412. Std of Reward: 0.094. Training.
[INFO] move_to_static_target_05. Step: 950000. Time Elapsed: 1246.752 s. Mean Reward: -55.412. Std of Reward: 0.094. Training.
[INFO] move_to_static_target_05. Step: 1000000. Time Elapsed: 1309.999 s. Mean Reward: -55.412. Std of Reward: 0.093. Training.
[INFO] Exported results/move_to_static_target_05/move_to_static_target_05/move_to_static_target_05-999984.onnx
